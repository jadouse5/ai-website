# The Website ðŸ¤– That Doesn't Exist ðŸª„ 

This project is a web server with a unique characteristic: **none of its pages exist as files**. Instead, it uses a Large Language Model (LLM) to generate and render entire web pages on the fly. Every request is routed through a configurable LLM provider, which creates the page content in real-time based on the URL you visit.

![Alt text](/demo.png)

## Table of Contents

1.  [Author](#author)
2.  [Introduction](#introduction)
3.  [Why Build a Website That Doesn't Exist?](#why-build-a-website-that-doesnt-exist)
4.  [Supported Models](#supported-models)
5.  [How It Works](#how-it-works)
6.  [Prerequisites](#prerequisites)
7.  [Installation](#installation)
8.  [Configuration](#configuration)
9.  [Running the Project](#running-the-project)
10. [License](#license)

## Author
This project was developed by **Jad Tounsi El Azzoiani** and anyone is free to clone it and use it for educational purposes only.

- **LinkedIn**: [Jad Tounsi El Azzoiani](https://www.linkedin.com/in/jad-tounsi-el-azzoiani-87499a21a/)

## Introduction

This project demonstrates a novel approach to web development where content is dynamically generated by an AI. The backend is a simple Python Flask server with a catch-all route. When a user navigates to any URL, the server constructs a prompt for the configured LLM, which then generates the complete HTML for that page. This allows for an infinitely scalable website where pages don't need to exist beforehand.

This is ideal for applications requiring:

* **Rapid prototyping** of website ideas.
* **Highly dynamic content** that changes frequently.
* **Personalized user experiences** where page content can be tailored to the user.

## Why Build a Website That Doesn't Exist?

This architectural approach, while unconventional, offers several powerful advantages over traditional static or template-based websites:

* **Infinite Content Scalability**: You can have a theoretically infinite number of pages without needing storage for each one. The only limit is your ability to describe a page in a URL.
* **Zero-Maintenance Content**: There are no individual HTML files to create, manage, or update. The "source of truth" for your website's content is the logic that generates the prompts, making site-wide changes incredibly efficient.
* **Ultimate Agility and Prototyping**: You can instantly visualize and test ideas for new web pages simply by navigating to a new URL. This is perfect for rapid prototyping and creative exploration without writing a single line of HTML.
* **Hyper-Dynamic Experiences**: Because content is generated on every request, it can be tailored in real-time. You could incorporate current events, user-specific data, or A/B test different content styles with minimal backend changes.

## Supported Models

This server is designed to be flexible and supports several LLM providers. You can easily switch between them:

* **Google Gemini**: A powerful and versatile model from Google.
* **Groq**: Known for its incredibly fast inference speeds using LPUs, supporting models like Llama and Mixtral.
* **Inception Labs Mercury**: A diffusion-based LLM that offers a different approach to text generation.
* **Qwen Coder**: A model specialized for code generation tasks.

## How It Works

The architecture is straightforward and relies on a few key components:

1.  **Flask Web Server**: A lightweight Python server to handle incoming HTTP requests.
2.  **Catch-All Route**: A single route (`/<path:path>`) captures all URL requests, making the server incredibly flexible.
3.  **Model Selection**: The server checks an environment variable to determine which LLM provider to use for the request.
4.  **Prompt Engineering**: The URL path from the request is used to create a descriptive prompt for the selected LLM. For example, a request to `/products/classic_cars` generates a prompt asking the LLM to create a product page for classic cars.
5.  **API Call**: The generated prompt is sent to the API of the chosen provider (Gemini, Groq, etc.).
6.  **Dynamic Rendering**: The LLM's response, which is a full HTML document, is sent directly back to the user's browser to be rendered.

## Prerequisites

Before running the project, ensure you have the following installed and configured:

* Python 3.9+
* **Flask** - for the web server.
* **SDKs for LLM Providers**:
    * `google-generativeai` for Google Gemini.
    * `groq` for Groq.
    * (Libraries for Mercury and Qwen as needed).
* **API Keys** for the services you intend to use.

## Installation

### Step 1: Clone the Repository

First, clone this repository to your local machine or create a project directory with the `main.py` and `requirements.txt` files.

### Step 2: Set Up a Virtual Environment

To keep project dependencies isolated, it is highly recommended to use a virtual environment.

1.  Open a terminal in your project directory.
2.  Create a virtual environment:
    ```bash
    python -m venv venv
    ```
3.  Activate the virtual environment:
    * **On macOS/Linux**:
        ```bash
        source venv/bin/activate
        ```
    * **On Windows**:
        ```bash
        venv\Scripts\activate
        ```

### Step 3: Install Dependencies

With the virtual environment active, install the required Python packages from the `requirements.txt` file.
```bash
pip install -r requirements.txt
```

### Step 4: Set Up API Keys

Set the API key for each service you want to use as an environment variable.

* **For Google Gemini**:
    ```bash
    export GOOGLE_API_KEY='your-google-api-key'
    ```
* **For Groq**:
    ```bash
    export GROQ_API_KEY='your-groq-api-key'
    ```
* **(Add similar instructions for Mercury and Qwen keys)**

## Configuration

To select which LLM provider to use, set the `LLM_PROVIDER` environment variable.

* **For Google Gemini**:
    ```bash
    export LLM_PROVIDER='GEMINI'
    ```
* **For Groq**:
    ```bash
    export LLM_PROVIDER='GROQ'
    ```
* **For Inception Labs Mercury**:
    ```bash
    export LLM_PROVIDER='MERCURY'
    ```
* **For Qwen Coder**:
    ```bash
    export LLM_PROVIDER='QWEN'
    ```

If `LLM_PROVIDER` is not set, the application will default to using Google Gemini.

## Running the Project

Once the setup is complete, you can run the web server.

1.  **Ensure your virtual environment is active**.
2.  **Set your desired `LLM_PROVIDER` and the corresponding API key**.
3.  **Run the Flask application**:
    ```bash
    python main.py
    ```
4.  **Access the app in your browser**:
    The server will start on `http://127.0.0.1:5000`. Open this URL in your browser.
5.  **Explore!**
    Try navigating to different paths to see the LLM generate pages on the fly. For example:
    * `http://127.0.0.1:5000/about_our_company`
    * `http://127.0.0.1:5000/products/solar_panels`
    * `http://127.0.0.1:5000/blog/a_trip_to_the_moon`

## License

This project is licensed under the MIT License - see the `LICENSE` file for details.
